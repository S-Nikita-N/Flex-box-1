{\rtf1\ansi\ansicpg1251\cocoartf2639
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fmodern\fcharset0 Courier;}
{\colortbl;\red255\green255\blue255;\red183\green111\blue179;\red23\green23\blue23;\red202\green202\blue202;
\red212\green212\blue212;\red89\green156\blue62;\red70\green137\blue204;\red212\green214\blue154;\red140\green211\blue254;
\red167\green197\blue152;\red67\green192\blue160;\red194\green126\blue101;\red113\green184\blue255;}
{\*\expandedcolortbl;;\cssrgb\c77255\c52549\c75294;\cssrgb\c11765\c11765\c11765;\cssrgb\c83137\c83137\c83137;
\cssrgb\c86275\c86275\c86275;\cssrgb\c41569\c66275\c30980;\cssrgb\c33725\c61176\c83922;\cssrgb\c86275\c86275\c66667;\cssrgb\c61176\c86275\c99608;
\cssrgb\c70980\c80784\c65882;\cssrgb\c30588\c78824\c69020;\cssrgb\c80784\c56863\c47059;\cssrgb\c50980\c77647\c100000;}
\paperw11900\paperh16840\margl1440\margr1440\vieww11520\viewh8400\viewkind0
\deftab720
\pard\pardeftab720\partightenfactor0

\f0\fs28 \cf2 \cb3 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 import\cf4 \strokec4  pandas \cf2 \strokec2 as\cf4 \strokec4  pd\cb1 \
\cf2 \cb3 \strokec2 import\cf4 \strokec4  numpy \cf2 \strokec2 as\cf4 \strokec4  np\cb1 \
\cf2 \cb3 \strokec2 import\cf4 \strokec4  random\cb1 \
\cf2 \cb3 \strokec2 import\cf4 \strokec4  torch\cb1 \
\cf2 \cb3 \strokec2 import\cf4 \strokec4  transformers\cb1 \
\cf2 \cb3 \strokec2 import\cf4 \strokec4  torch.nn \cf2 \strokec2 as\cf4 \strokec4  nn\cb1 \
\cf2 \cb3 \strokec2 from\cf4 \strokec4  transformers \cf2 \strokec2 import\cf4 \strokec4  AutoModel\cf5 \strokec5 ,\cf4 \strokec4  AutoModelForSequenceClassification\cf5 \strokec5 ,\cf4 \strokec4  AutoTokenizer\cb1 \
\pard\pardeftab720\partightenfactor0
\cf6 \cb3 \strokec6 # from transformers import TrainingArguments, Trainer\cf4 \cb1 \strokec4 \
\cf6 \cb3 \strokec6 # from datasets import load_metric, Dataset\cf4 \cb1 \strokec4 \
\pard\pardeftab720\partightenfactor0
\cf2 \cb3 \strokec2 from\cf4 \strokec4  sklearn.metrics \cf2 \strokec2 import\cf4 \strokec4  classification_report\cf5 \strokec5 ,\cf4 \strokec4  f1_score\cb1 \
\cf2 \cb3 \strokec2 from\cf4 \strokec4  tqdm.auto \cf2 \strokec2 import\cf4 \strokec4  tqdm\cb1 \
\cf2 \cb3 \strokec2 from\cf4 \strokec4  torch.utils.data \cf2 \strokec2 import\cf4 \strokec4  DataLoader\cb1 \
\cf2 \cb3 \strokec2 from\cf4 \strokec4  sklearn.model_selection \cf2 \strokec2 import\cf4 \strokec4  train_test_split\cb1 \
\cf2 \cb3 \strokec2 from\cf4 \strokec4  sklearn.metrics \cf2 \strokec2 import\cf4 \strokec4  roc_auc_score\cb1 \
\cf2 \cb3 \strokec2 from\cf4 \strokec4  torch.optim \cf2 \strokec2 import\cf4 \strokec4  AdamW\cb1 \
\cf2 \cb3 \strokec2 import\cf4 \strokec4  gc\cb1 \
\cf2 \cb3 \strokec2 from\cf4 \strokec4  transformers \cf2 \strokec2 import\cf4 \strokec4  get_scheduler\
\pard\pardeftab720\partightenfactor0
\cf2 \strokec2 from\cf4 \strokec4  transformers \cf2 \strokec2 import\cf4 \strokec4  BertTokenizer\cb1 \
\cf2 \cb3 \strokec2 from\cf4 \strokec4  transformers \cf2 \strokec2 import\cf4 \strokec4  BertForSequenceClassification\cb1 \
\cf2 \cb3 \strokec2 from\cf4 \strokec4  transformers \cf2 \strokec2 import\cf4 \strokec4  get_linear_schedule_with_warmup\cb1 \
\cf2 \cb3 \strokec2 from\cf4 \strokec4  torch.utils.data \cf2 \strokec2 import\cf4 \strokec4  DataLoader\cb1 \
\cf2 \cb3 \strokec2 from\cf4 \strokec4  transformers \cf2 \strokec2 import\cf4 \strokec4  AdamW\cb1 \
\pard\pardeftab720\partightenfactor0
\cf4 \
\
\pard\pardeftab720\partightenfactor0
\cf7 \cb3 \strokec7 def\cf4 \cb3 \strokec4  \cf8 \cb3 \strokec8 gini\cf4 \cb3 \strokec4 (\cf9 \cb3 \strokec9 labels\cf4 \cb3 \strokec4 , \cf9 \cb3 \strokec9 preds\cf4 \cb3 \strokec4 )\cf5 \strokec5 :\cf4 \cb1 \strokec4 \
\pard\pardeftab720\partightenfactor0
\cf4 \cb3     gini = \cf10 \cb3 \strokec10 2\cf4 \cb3 \strokec4 * roc_auc_score\cf5 \strokec5 (\cf4 \strokec4 labels\cf5 \strokec5 ,\cf4 \strokec4  preds\cf5 \strokec5 )\cf4 \strokec4  - \cf10 \cb3 \strokec10 1\cf4 \cb1 \strokec4 \
\cb3     \cf2 \strokec2 return\cf4 \strokec4  gini\
\pard\pardeftab720\partightenfactor0
\cf4 \cb1 \
\
\pard\pardeftab720\partightenfactor0
\cf2 \cb3 \strokec2 from\cf4 \strokec4  torch.utils.data \cf2 \strokec2 import\cf4 \strokec4  Dataset\cb1 \
\
\pard\pardeftab720\partightenfactor0
\cf7 \cb3 \strokec7 class\cf4 \cb3 \strokec4  \cf11 \cb3 \strokec11 CustomDataset\cf4 \cb3 \strokec4 (\cf11 \cb3 \strokec11 Dataset\cf4 \cb3 \strokec4 )\cf5 \strokec5 :\cf4 \cb1 \strokec4 \
\
\pard\pardeftab720\partightenfactor0
\cf4 \cb3     \cf7 \cb3 \strokec7 def\cf4 \cb3 \strokec4  \cf8 \cb3 \strokec8 __init__\cf4 \cb3 \strokec4 (\cf9 \cb3 \strokec9 self\cf4 \cb3 \strokec4 , \cf9 \cb3 \strokec9 texts\cf4 \cb3 \strokec4 , \cf9 \cb3 \strokec9 targets\cf4 \cb3 \strokec4 , \cf9 \cb3 \strokec9 tokenizer\cf4 \cb3 \strokec4 , \cf9 \cb3 \strokec9 max_len\cf4 \cb3 \strokec4 =\cf10 \cb3 \strokec10 512\cf4 \cb3 \strokec4 )\cf5 \strokec5 :\cf4 \cb1 \strokec4 \
\cb3         \cf9 \cb3 \strokec9 self\cf4 \cb3 \strokec4 .texts = texts\cb1 \
\cb3         \cf9 \cb3 \strokec9 self\cf4 \cb3 \strokec4 .targets = targets\cb1 \
\cb3         \cf9 \cb3 \strokec9 self\cf4 \cb3 \strokec4 .tokenizer = tokenizer\cb1 \
\cb3         \cf9 \cb3 \strokec9 self\cf4 \cb3 \strokec4 .max_len = max_len\cb1 \
\
\cb3     \cf7 \cb3 \strokec7 def\cf4 \cb3 \strokec4  \cf8 \cb3 \strokec8 __len__\cf4 \cb3 \strokec4 (\cf9 \cb3 \strokec9 self\cf4 \cb3 \strokec4 )\cf5 \strokec5 :\cf4 \cb1 \strokec4 \
\cb3         \cf2 \strokec2 return\cf4 \strokec4  \cf8 \cb3 \strokec8 len\cf5 \cb3 \strokec5 (\cf9 \cb3 \strokec9 self\cf4 \cb3 \strokec4 .texts\cf5 \strokec5 )\cf4 \cb1 \strokec4 \
\
\cb3     \cf7 \cb3 \strokec7 def\cf4 \cb3 \strokec4  \cf8 \cb3 \strokec8 __getitem__\cf4 \cb3 \strokec4 (\cf9 \cb3 \strokec9 self\cf4 \cb3 \strokec4 , \cf9 \cb3 \strokec9 idx\cf4 \cb3 \strokec4 )\cf5 \strokec5 :\cf4 \cb1 \strokec4 \
\cb3         text = \cf11 \cb3 \strokec11 str\cf5 \cb3 \strokec5 (\cf9 \cb3 \strokec9 self\cf4 \cb3 \strokec4 .texts\cf5 \strokec5 [\cf4 \strokec4 idx\cf5 \strokec5 ])\cf4 \cb1 \strokec4 \
\cb3         target = \cf9 \cb3 \strokec9 self\cf4 \cb3 \strokec4 .targets\cf5 \strokec5 [\cf4 \strokec4 idx\cf5 \strokec5 ]\cf4 \cb1 \strokec4 \
\
\cb3         encoding = \cf9 \cb3 \strokec9 self\cf4 \cb3 \strokec4 .tokenizer.encode_plus\cf5 \strokec5 (\cf4 \cb1 \strokec4 \
\cb3             text\cf5 \strokec5 ,\cf4 \cb1 \strokec4 \
\cb3             add_special_tokens=\cf7 \cb3 \strokec7 True\cf5 \cb3 \strokec5 ,\cf4 \cb1 \strokec4 \
\cb3             return_token_type_ids=\cf7 \cb3 \strokec7 False\cf5 \cb3 \strokec5 ,\cf4 \cb1 \strokec4 \
\cb3             return_attention_mask=\cf7 \cb3 \strokec7 True\cf5 \cb3 \strokec5 ,\cf4 \cb1 \strokec4 \
\cb3             max_length = \cf10 \cb3 \strokec10 512\cf5 \cb3 \strokec5 ,\cf4 \cb1 \strokec4 \
\cb3             padding = \cf12 \cb3 \strokec12 'max_length'\cf5 \cb3 \strokec5 ,\cf4 \cb1 \strokec4 \
\cb3             truncation = \cf7 \cb3 \strokec7 True\cf5 \cb3 \strokec5 ,\cf4 \cb1 \strokec4 \
\cb3             return_tensors=\cf12 \cb3 \strokec12 'pt'\cf5 \cb3 \strokec5 ,\cf4 \cb1 \strokec4 \
\cb3         \cf5 \strokec5 )\cf4 \cb1 \strokec4 \
\
\cb3         \cf2 \strokec2 return\cf4 \strokec4  \cf5 \strokec5 \{\cf4 \cb1 \strokec4 \
\cb3           \cf12 \cb3 \strokec12 'text'\cf5 \cb3 \strokec5 :\cf4 \strokec4  text\cf5 \strokec5 ,\cf4 \cb1 \strokec4 \
\cb3           \cf12 \cb3 \strokec12 'input_ids'\cf5 \cb3 \strokec5 :\cf4 \strokec4  encoding\cf5 \strokec5 [\cf12 \cb3 \strokec12 'input_ids'\cf5 \cb3 \strokec5 ]\cf4 \strokec4 .flatten\cf5 \strokec5 (),\cf4 \cb1 \strokec4 \
\cb3           \cf12 \cb3 \strokec12 'attention_mask'\cf5 \cb3 \strokec5 :\cf4 \strokec4  encoding\cf5 \strokec5 [\cf12 \cb3 \strokec12 'attention_mask'\cf5 \cb3 \strokec5 ]\cf4 \strokec4 .flatten\cf5 \strokec5 (),\cf4 \cb1 \strokec4 \
\cb3           \cf12 \cb3 \strokec12 'targets'\cf5 \cb3 \strokec5 :\cf4 \strokec4  torch.tensor\cf5 \strokec5 (\cf4 \strokec4 target\cf5 \strokec5 ,\cf4 \strokec4  dtype=torch.\cf9 \cb3 \strokec9 long\cf5 \cb3 \strokec5 )\cf4 \cb1 \strokec4 \
\cb3         \cf5 \strokec5 \}\
\
\
\pard\pardeftab720\partightenfactor0
\cf7 \cb3 \strokec7 class\cf4 \cb3 \strokec4  BertClassifier\cf5 \strokec5 :\cf4 \cb1 \strokec4 \
\
\pard\pardeftab720\partightenfactor0
\cf4 \cb3     \cf7 \cb3 \strokec7 def\cf4 \cb3 \strokec4  \cf8 \cb3 \strokec8 __init__\cf4 \cb3 \strokec4 (\cf9 \cb3 \strokec9 self\cf4 \cb3 \strokec4 , \cf9 \cb3 \strokec9 model_path\cf4 \cb3 \strokec4 , \cf9 \cb3 \strokec9 tokenizer_path\cf4 \cb3 \strokec4 , \cf9 \cb3 \strokec9 n_classes\cf4 \cb3 \strokec4 =\cf10 \cb3 \strokec10 2\cf4 \cb3 \strokec4 , \cf9 \cb3 \strokec9 epochs\cf4 \cb3 \strokec4 =\cf10 \cb3 \strokec10 1\cf4 \cb3 \strokec4 , \cf9 \cb3 \strokec9 model_save_path\cf4 \cb3 \strokec4 =\cf12 \cb3 \strokec12 '/content/bert.pt'\cf4 \cb3 \strokec4 )\cf5 \strokec5 :\cf4 \cb1 \strokec4 \
\cb3         \cf9 \cb3 \strokec9 self\cf4 \cb3 \strokec4 .model = BertForSequenceClassification.from_pretrained\cf5 \strokec5 (\cf4 \strokec4 model_path\cf5 \strokec5 )\cf4 \cb1 \strokec4 \
\cb3         \cf9 \cb3 \strokec9 self\cf4 \cb3 \strokec4 .tokenizer = BertTokenizer.from_pretrained\cf5 \strokec5 (\cf4 \strokec4 tokenizer_path\cf5 \strokec5 )\cf4 \cb1 \strokec4 \
\cb3         \cf9 \cb3 \strokec9 self\cf4 \cb3 \strokec4 .device = torch.device\cf5 \strokec5 (\cf12 \cb3 \strokec12 "cuda:0"\cf4 \cb3 \strokec4  \cf2 \strokec2 if\cf4 \strokec4  torch.cuda.is_available\cf5 \strokec5 ()\cf4 \strokec4  \cf2 \strokec2 else\cf4 \strokec4  \cf12 \cb3 \strokec12 "cpu"\cf5 \cb3 \strokec5 )\cf4 \cb1 \strokec4 \
\cb3         \cf9 \cb3 \strokec9 self\cf4 \cb3 \strokec4 .model_save_path=model_save_path\cb1 \
\cb3         \cf9 \cb3 \strokec9 self\cf4 \cb3 \strokec4 .max_len = \cf10 \cb3 \strokec10 512\cf4 \cb1 \strokec4 \
\cb3         \cf9 \cb3 \strokec9 self\cf4 \cb3 \strokec4 .epochs = epochs\cb1 \
\cb3         \cf9 \cb3 \strokec9 self\cf4 \cb3 \strokec4 .out_features = \cf9 \cb3 \strokec9 self\cf4 \cb3 \strokec4 .model.bert.encoder.layer\cf5 \strokec5 [\cf10 \cb3 \strokec10 1\cf5 \cb3 \strokec5 ]\cf4 \strokec4 .output.dense.out_features\cb1 \
\cb3         \cf9 \cb3 \strokec9 self\cf4 \cb3 \strokec4 .model.classifier = torch.nn.Linear\cf5 \strokec5 (\cf9 \cb3 \strokec9 self\cf4 \cb3 \strokec4 .out_features\cf5 \strokec5 ,\cf4 \strokec4  n_classes\cf5 \strokec5 )\cf4 \cb1 \strokec4 \
\cb3         \cf9 \cb3 \strokec9 self\cf4 \cb3 \strokec4 .model.to\cf5 \strokec5 (\cf9 \cb3 \strokec9 self\cf4 \cb3 \strokec4 .device\cf5 \strokec5 )\cf4 \cb1 \strokec4 \
\cb3     \cb1 \
\cb3     \cb1 \
\cb3     \cf7 \cb3 \strokec7 def\cf4 \cb3 \strokec4  \cf8 \cb3 \strokec8 preparation\cf4 \cb3 \strokec4 (\cf9 \cb3 \strokec9 self\cf4 \cb3 \strokec4 , \cf9 \cb3 \strokec9 X_train\cf4 \cb3 \strokec4 , \cf9 \cb3 \strokec9 y_train\cf4 \cb3 \strokec4 , \cf9 \cb3 \strokec9 X_valid\cf4 \cb3 \strokec4 , \cf9 \cb3 \strokec9 y_valid\cf4 \cb3 \strokec4 )\cf5 \strokec5 :\cf4 \cb1 \strokec4 \
\cb3         \cf6 \strokec6 # create datasets\cf4 \cb1 \strokec4 \
\cb3         \cf9 \cb3 \strokec9 self\cf4 \cb3 \strokec4 .train_set = CustomDataset\cf5 \strokec5 (\cf4 \strokec4 X_train\cf5 \strokec5 ,\cf4 \strokec4  y_train\cf5 \strokec5 ,\cf4 \strokec4  \cf9 \cb3 \strokec9 self\cf4 \cb3 \strokec4 .tokenizer\cf5 \strokec5 )\cf4 \cb1 \strokec4 \
\cb3         \cf9 \cb3 \strokec9 self\cf4 \cb3 \strokec4 .valid_set = CustomDataset\cf5 \strokec5 (\cf4 \strokec4 X_valid\cf5 \strokec5 ,\cf4 \strokec4  y_valid\cf5 \strokec5 ,\cf4 \strokec4  \cf9 \cb3 \strokec9 self\cf4 \cb3 \strokec4 .tokenizer\cf5 \strokec5 )\cf4 \cb1 \strokec4 \
\
\cb3         \cf6 \strokec6 # create data loaders\cf4 \cb1 \strokec4 \
\cb3         \cf9 \cb3 \strokec9 self\cf4 \cb3 \strokec4 .train_loader = DataLoader\cf5 \strokec5 (\cf9 \cb3 \strokec9 self\cf4 \cb3 \strokec4 .train_set\cf5 \strokec5 ,\cf4 \strokec4  batch_size=\cf10 \cb3 \strokec10 2\cf5 \cb3 \strokec5 ,\cf4 \strokec4  shuffle=\cf7 \cb3 \strokec7 True\cf5 \cb3 \strokec5 )\cf4 \cb1 \strokec4 \
\cb3         \cf9 \cb3 \strokec9 self\cf4 \cb3 \strokec4 .valid_loader = DataLoader\cf5 \strokec5 (\cf9 \cb3 \strokec9 self\cf4 \cb3 \strokec4 .valid_set\cf5 \strokec5 ,\cf4 \strokec4  batch_size=\cf10 \cb3 \strokec10 2\cf5 \cb3 \strokec5 ,\cf4 \strokec4  shuffle=\cf7 \cb3 \strokec7 True\cf5 \cb3 \strokec5 )\cf4 \cb1 \strokec4 \
\
\cb3         \cf6 \strokec6 # helpers initialization\cf4 \cb1 \strokec4 \
\cb3         \cf9 \cb3 \strokec9 self\cf4 \cb3 \strokec4 .optimizer = AdamW\cf5 \strokec5 (\cf9 \cb3 \strokec9 self\cf4 \cb3 \strokec4 .model.parameters\cf5 \strokec5 (),\cf4 \strokec4  lr=\cf10 \cb3 \strokec10 2e-5\cf5 \cb3 \strokec5 ,\cf4 \strokec4  correct_bias=\cf7 \cb3 \strokec7 False\cf5 \cb3 \strokec5 )\cf4 \cb1 \strokec4 \
\cb3         \cf9 \cb3 \strokec9 self\cf4 \cb3 \strokec4 .scheduler = get_linear_schedule_with_warmup\cf5 \strokec5 (\cf4 \cb1 \strokec4 \
\cb3                 \cf9 \cb3 \strokec9 self\cf4 \cb3 \strokec4 .optimizer\cf5 \strokec5 ,\cf4 \cb1 \strokec4 \
\cb3                 num_warmup_steps=\cf10 \cb3 \strokec10 0\cf5 \cb3 \strokec5 ,\cf4 \cb1 \strokec4 \
\cb3                 num_training_steps=\cf8 \cb3 \strokec8 len\cf5 \cb3 \strokec5 (\cf9 \cb3 \strokec9 self\cf4 \cb3 \strokec4 .train_loader\cf5 \strokec5 )\cf4 \strokec4  * \cf9 \cb3 \strokec9 self\cf4 \cb3 \strokec4 .epochs\cb1 \
\cb3             \cf5 \strokec5 )\cf4 \cb1 \strokec4 \
\cb3         \cf9 \cb3 \strokec9 self\cf4 \cb3 \strokec4 .loss_fn = torch.nn.CrossEntropyLoss\cf5 \strokec5 ()\cf4 \strokec4 .to\cf5 \strokec5 (\cf9 \cb3 \strokec9 self\cf4 \cb3 \strokec4 .device\cf5 \strokec5 )\cf4 \cb1 \strokec4 \
\cb3     \cb1 \
\cb3     \cf7 \cb3 \strokec7 def\cf4 \cb3 \strokec4  \cf8 \cb3 \strokec8 fit\cf4 \cb3 \strokec4 (\cf9 \cb3 \strokec9 self\cf4 \cb3 \strokec4 )\cf5 \strokec5 :\cf4 \cb1 \strokec4 \
\cb3         \cf9 \cb3 \strokec9 self\cf4 \cb3 \strokec4 .model = \cf9 \cb3 \strokec9 self\cf4 \cb3 \strokec4 .model.train\cf5 \strokec5 ()\cf4 \cb1 \strokec4 \
\cb3         losses = \cf5 \strokec5 []\cf4 \cb1 \strokec4 \
\cb3         correct_predictions = \cf10 \cb3 \strokec10 0\cf4 \cb1 \strokec4 \
\cb3         preds = \cf5 \strokec5 []\cf4 \cb1 \strokec4 \
\cb3         y_true = \cf5 \strokec5 []\cf4 \cb1 \strokec4 \
\cb3         progress_bar = tqdm\cf5 \strokec5 (\cf8 \cb3 \strokec8 range\cf5 \cb3 \strokec5 (\cf8 \cb3 \strokec8 len\cf5 \cb3 \strokec5 (\cf9 \cb3 \strokec9 self\cf4 \cb3 \strokec4 .train_loader\cf5 \strokec5 )))\cf4 \cb1 \strokec4 \
\cb3         \cf2 \strokec2 for\cf4 \strokec4  data \cf13 \cb3 \strokec13 in\cf4 \cb3 \strokec4  \cf9 \cb3 \strokec9 self\cf4 \cb3 \strokec4 .train_loader\cf5 \strokec5 :\cf4 \cb1 \strokec4 \
\cb3             input_ids = data\cf5 \strokec5 [\cf12 \cb3 \strokec12 "input_ids"\cf5 \cb3 \strokec5 ]\cf4 \strokec4 .to\cf5 \strokec5 (\cf9 \cb3 \strokec9 self\cf4 \cb3 \strokec4 .device\cf5 \strokec5 )\cf4 \cb1 \strokec4 \
\cb3             attention_mask = data\cf5 \strokec5 [\cf12 \cb3 \strokec12 "attention_mask"\cf5 \cb3 \strokec5 ]\cf4 \strokec4 .to\cf5 \strokec5 (\cf9 \cb3 \strokec9 self\cf4 \cb3 \strokec4 .device\cf5 \strokec5 )\cf4 \cb1 \strokec4 \
\cb3             targets = data\cf5 \strokec5 [\cf12 \cb3 \strokec12 "targets"\cf5 \cb3 \strokec5 ]\cf4 \strokec4 .to\cf5 \strokec5 (\cf9 \cb3 \strokec9 self\cf4 \cb3 \strokec4 .device\cf5 \strokec5 )\cf4 \cb1 \strokec4 \
\
\cb3             outputs = \cf9 \cb3 \strokec9 self\cf4 \cb3 \strokec4 .model\cf5 \strokec5 (\cf4 \cb1 \strokec4 \
\cb3                 input_ids=input_ids\cf5 \strokec5 ,\cf4 \cb1 \strokec4 \
\cb3                 attention_mask=attention_mask\cb1 \
\cb3                 \cf5 \strokec5 )\cf4 \cb1 \strokec4 \
\
\cb3             \cf6 \strokec6 # preds = torch.argmax(outputs.logits, dim=1)\cf4 \cb1 \strokec4 \
\cb3             logits = \cf5 \strokec5 (\cf4 \strokec4 np.e**outputs.logits\cf5 \strokec5 [:,\cf10 \cb3 \strokec10 1\cf5 \cb3 \strokec5 ]\cf4 \strokec4  / torch.\cf8 \cb3 \strokec8 sum\cf5 \cb3 \strokec5 (\cf4 \strokec4 np.e**outputs.logits\cf5 \strokec5 ,\cf4 \strokec4  axis = \cf10 \cb3 \strokec10 1\cf5 \cb3 \strokec5 ))\cf4 \strokec4 .detach\cf5 \strokec5 ()\cf4 \strokec4 .cpu\cf5 \strokec5 ()\cf4 \strokec4 .tolist\cf5 \strokec5 ()\cf4 \cb1 \strokec4 \
\cb3             preds.extend\cf5 \strokec5 (\cf4 \strokec4 logits\cf5 \strokec5 )\cf4 \cb1 \strokec4 \
\cb3             y_true.extend\cf5 \strokec5 (\cf4 \strokec4 targets.detach\cf5 \strokec5 ()\cf4 \strokec4 .cpu\cf5 \strokec5 ()\cf4 \strokec4 .tolist\cf5 \strokec5 ())\cf4 \cb1 \strokec4 \
\
\cb3             loss = \cf9 \cb3 \strokec9 self\cf4 \cb3 \strokec4 .loss_fn\cf5 \strokec5 (\cf4 \strokec4 outputs.logits\cf5 \strokec5 ,\cf4 \strokec4  targets\cf5 \strokec5 )\cf4 \cb1 \strokec4 \
\cb3             losses.append\cf5 \strokec5 (\cf4 \strokec4 loss.item\cf5 \strokec5 ())\cf4 \cb1 \strokec4 \
\cb3             \cf6 \strokec6 # correct_predictions += torch.sum(preds == targets)\cf4 \cb1 \strokec4 \
\
\cb3             \cb1 \
\
\cb3             loss.backward\cf5 \strokec5 ()\cf4 \cb1 \strokec4 \
\cb3             torch.nn.utils.clip_grad_norm_\cf5 \strokec5 (\cf9 \cb3 \strokec9 self\cf4 \cb3 \strokec4 .model.parameters\cf5 \strokec5 (),\cf4 \strokec4  max_norm=\cf10 \cb3 \strokec10 1.0\cf5 \cb3 \strokec5 )\cf4 \cb1 \strokec4 \
\cb3             \cf9 \cb3 \strokec9 self\cf4 \cb3 \strokec4 .optimizer.step\cf5 \strokec5 ()\cf4 \cb1 \strokec4 \
\cb3             \cf9 \cb3 \strokec9 self\cf4 \cb3 \strokec4 .scheduler.step\cf5 \strokec5 ()\cf4 \cb1 \strokec4 \
\cb3             \cf9 \cb3 \strokec9 self\cf4 \cb3 \strokec4 .optimizer.zero_grad\cf5 \strokec5 ()\cf4 \cb1 \strokec4 \
\
\cb3             progress_bar.update\cf5 \strokec5 (\cf10 \cb3 \strokec10 1\cf5 \cb3 \strokec5 )\cf4 \cb1 \strokec4 \
\
\cb3         \cf6 \strokec6 # train_acc = correct_predictions.double() / len(self.train_set)\cf4 \cb1 \strokec4 \
\cb3         train_gini = gini\cf5 \strokec5 (\cf4 \strokec4 y_true\cf5 \strokec5 ,\cf4 \strokec4  preds\cf5 \strokec5 )\cf4 \cb1 \strokec4 \
\cb3         train_loss = np.mean\cf5 \strokec5 (\cf4 \strokec4 losses\cf5 \strokec5 )\cf4 \cb1 \strokec4 \
\cb3         \cb1 \
\cb3         \cf2 \strokec2 return\cf4 \strokec4  train_gini\cf5 \strokec5 ,\cf4 \strokec4  train_loss\cb1 \
\
\cb3     \cf7 \cb3 \strokec7 def\cf4 \cb3 \strokec4  \cf8 \cb3 \strokec8 eval\cf4 \cb3 \strokec4 (\cf9 \cb3 \strokec9 self\cf4 \cb3 \strokec4 )\cf5 \strokec5 :\cf4 \cb1 \strokec4 \
\cb3         \cf9 \cb3 \strokec9 self\cf4 \cb3 \strokec4 .model = \cf9 \cb3 \strokec9 self\cf4 \cb3 \strokec4 .model.\cf8 \cb3 \strokec8 eval\cf5 \cb3 \strokec5 ()\cf4 \cb1 \strokec4 \
\cb3         losses = \cf5 \strokec5 []\cf4 \cb1 \strokec4 \
\cb3         preds = \cf5 \strokec5 []\cf4 \cb1 \strokec4 \
\cb3         y_true = \cf5 \strokec5 []\cf4 \cb1 \strokec4 \
\cb3         \cf6 \strokec6 # correct_predictions = 0\cf4 \cb1 \strokec4 \
\cb3         progress_bar = tqdm\cf5 \strokec5 (\cf8 \cb3 \strokec8 range\cf5 \cb3 \strokec5 (\cf8 \cb3 \strokec8 len\cf5 \cb3 \strokec5 (\cf9 \cb3 \strokec9 self\cf4 \cb3 \strokec4 .valid_loader\cf5 \strokec5 )))\cf4 \cb1 \strokec4 \
\cb3         \cf2 \strokec2 with\cf4 \strokec4  torch.no_grad\cf5 \strokec5 ():\cf4 \cb1 \strokec4 \
\cb3             \cf2 \strokec2 for\cf4 \strokec4  data \cf13 \cb3 \strokec13 in\cf4 \cb3 \strokec4  \cf9 \cb3 \strokec9 self\cf4 \cb3 \strokec4 .valid_loader\cf5 \strokec5 :\cf4 \cb1 \strokec4 \
\cb3                 input_ids = data\cf5 \strokec5 [\cf12 \cb3 \strokec12 "input_ids"\cf5 \cb3 \strokec5 ]\cf4 \strokec4 .to\cf5 \strokec5 (\cf9 \cb3 \strokec9 self\cf4 \cb3 \strokec4 .device\cf5 \strokec5 )\cf4 \cb1 \strokec4 \
\cb3                 attention_mask = data\cf5 \strokec5 [\cf12 \cb3 \strokec12 "attention_mask"\cf5 \cb3 \strokec5 ]\cf4 \strokec4 .to\cf5 \strokec5 (\cf9 \cb3 \strokec9 self\cf4 \cb3 \strokec4 .device\cf5 \strokec5 )\cf4 \cb1 \strokec4 \
\cb3                 targets = data\cf5 \strokec5 [\cf12 \cb3 \strokec12 "targets"\cf5 \cb3 \strokec5 ]\cf4 \strokec4 .to\cf5 \strokec5 (\cf9 \cb3 \strokec9 self\cf4 \cb3 \strokec4 .device\cf5 \strokec5 )\cf4 \cb1 \strokec4 \
\
\cb3                 outputs = \cf9 \cb3 \strokec9 self\cf4 \cb3 \strokec4 .model\cf5 \strokec5 (\cf4 \cb1 \strokec4 \
\cb3                     input_ids=input_ids\cf5 \strokec5 ,\cf4 \cb1 \strokec4 \
\cb3                     attention_mask=attention_mask\cb1 \
\cb3                     \cf5 \strokec5 )\cf4 \cb1 \strokec4 \
\
\cb3                 \cf6 \strokec6 # preds = torch.argmax(outputs.logits, dim=1)\cf4 \cb1 \strokec4 \
\cb3                 logits = \cf5 \strokec5 (\cf4 \strokec4 np.e**outputs.logits\cf5 \strokec5 [:,\cf10 \cb3 \strokec10 1\cf5 \cb3 \strokec5 ]\cf4 \strokec4  / torch.\cf8 \cb3 \strokec8 sum\cf5 \cb3 \strokec5 (\cf4 \strokec4 np.e**outputs.logits\cf5 \strokec5 ,\cf4 \strokec4  axis = \cf10 \cb3 \strokec10 1\cf5 \cb3 \strokec5 ))\cf4 \strokec4 .detach\cf5 \strokec5 ()\cf4 \strokec4 .cpu\cf5 \strokec5 ()\cf4 \strokec4 .tolist\cf5 \strokec5 ()\cf4 \cb1 \strokec4 \
\cb3                 preds.extend\cf5 \strokec5 (\cf4 \strokec4 logits\cf5 \strokec5 )\cf4 \cb1 \strokec4 \
\cb3                 y_true.extend\cf5 \strokec5 (\cf4 \strokec4 targets.detach\cf5 \strokec5 ()\cf4 \strokec4 .cpu\cf5 \strokec5 ()\cf4 \strokec4 .tolist\cf5 \strokec5 ())\cf4 \cb1 \strokec4 \
\
\cb3                 loss = \cf9 \cb3 \strokec9 self\cf4 \cb3 \strokec4 .loss_fn\cf5 \strokec5 (\cf4 \strokec4 outputs.logits\cf5 \strokec5 ,\cf4 \strokec4  targets\cf5 \strokec5 )\cf4 \cb1 \strokec4 \
\cb3                 losses.append\cf5 \strokec5 (\cf4 \strokec4 loss.item\cf5 \strokec5 ())\cf4 \cb1 \strokec4 \
\cb3                 \cf6 \strokec6 # correct_predictions += torch.sum(preds == targets)\cf4 \cb1 \strokec4 \
\cb3                 \cb1 \
\cb3                 progress_bar.update\cf5 \strokec5 (\cf10 \cb3 \strokec10 1\cf5 \cb3 \strokec5 )\cf4 \cb1 \strokec4 \
\
\cb3         \cf6 \strokec6 # val_acc = correct_predictions.double() / len(self.valid_set)\cf4 \cb1 \strokec4 \
\
\cb3         val_gini = gini\cf5 \strokec5 (\cf4 \strokec4 y_true\cf5 \strokec5 ,\cf4 \strokec4  preds\cf5 \strokec5 )\cf4 \cb1 \strokec4 \
\cb3         val_loss = np.mean\cf5 \strokec5 (\cf4 \strokec4 losses\cf5 \strokec5 )\cf4 \cb1 \strokec4 \
\
\cb3         \cf2 \strokec2 return\cf4 \strokec4  val_gini\cf5 \strokec5 ,\cf4 \strokec4  val_loss\cb1 \
\cb3     \cb1 \
\cb3     \cf7 \cb3 \strokec7 def\cf4 \cb3 \strokec4  \cf8 \cb3 \strokec8 train\cf4 \cb3 \strokec4 (\cf9 \cb3 \strokec9 self\cf4 \cb3 \strokec4 )\cf5 \strokec5 :\cf4 \cb1 \strokec4 \
\cb3         best_accuracy = \cf10 \cb3 \strokec10 0\cf4 \cb1 \strokec4 \
\cb3         \cb1 \
\cb3         \cf2 \strokec2 for\cf4 \strokec4  epoch \cf13 \cb3 \strokec13 in\cf4 \cb3 \strokec4  \cf8 \cb3 \strokec8 range\cf5 \cb3 \strokec5 (\cf9 \cb3 \strokec9 self\cf4 \cb3 \strokec4 .epochs\cf5 \strokec5 ):\cf4 \cb1 \strokec4 \
\cb3             \cf8 \cb3 \strokec8 print\cf5 \cb3 \strokec5 (\cf7 \cb3 \strokec7 f\cf12 \cb3 \strokec12 'Epoch \cf5 \cb3 \strokec5 \{\cf4 \strokec4 epoch + \cf10 \cb3 \strokec10 1\cf5 \cb3 \strokec5 \}\cf12 \cb3 \strokec12 /\cf5 \cb3 \strokec5 \{\cf9 \cb3 \strokec9 self\cf4 \cb3 \strokec4 .epochs\cf5 \strokec5 \}\cf12 \cb3 \strokec12 '\cf5 \cb3 \strokec5 )\cf4 \cb1 \strokec4 \
\cb3             train_acc\cf5 \strokec5 ,\cf4 \strokec4  train_loss = \cf9 \cb3 \strokec9 self\cf4 \cb3 \strokec4 .fit\cf5 \strokec5 ()\cf4 \cb1 \strokec4 \
\cb3             \cf8 \cb3 \strokec8 print\cf5 \cb3 \strokec5 (\cf7 \cb3 \strokec7 f\cf12 \cb3 \strokec12 'Train loss \cf5 \cb3 \strokec5 \{\cf4 \strokec4 train_loss\cf5 \strokec5 \}\cf12 \cb3 \strokec12  accuracy \cf5 \cb3 \strokec5 \{\cf4 \strokec4 train_acc\cf5 \strokec5 \}\cf12 \cb3 \strokec12 '\cf5 \cb3 \strokec5 )\cf4 \cb1 \strokec4 \
\
\cb3             val_acc\cf5 \strokec5 ,\cf4 \strokec4  val_loss = \cf9 \cb3 \strokec9 self\cf4 \cb3 \strokec4 .\cf8 \cb3 \strokec8 eval\cf5 \cb3 \strokec5 ()\cf4 \cb1 \strokec4 \
\cb3             \cf8 \cb3 \strokec8 print\cf5 \cb3 \strokec5 (\cf7 \cb3 \strokec7 f\cf12 \cb3 \strokec12 'Val loss \cf5 \cb3 \strokec5 \{\cf4 \strokec4 val_loss\cf5 \strokec5 \}\cf12 \cb3 \strokec12  accuracy \cf5 \cb3 \strokec5 \{\cf4 \strokec4 val_acc\cf5 \strokec5 \}\cf12 \cb3 \strokec12 '\cf5 \cb3 \strokec5 )\cf4 \cb1 \strokec4 \
\cb3             \cf8 \cb3 \strokec8 print\cf5 \cb3 \strokec5 (\cf12 \cb3 \strokec12 '-'\cf4 \cb3 \strokec4  * \cf10 \cb3 \strokec10 10\cf5 \cb3 \strokec5 )\cf4 \cb1 \strokec4 \
\
\cb3             \cf2 \strokec2 if\cf4 \strokec4  val_acc > best_accuracy\cf5 \strokec5 :\cf4 \cb1 \strokec4 \
\cb3                 torch.save\cf5 \strokec5 (\cf9 \cb3 \strokec9 self\cf4 \cb3 \strokec4 .model\cf5 \strokec5 ,\cf4 \strokec4  \cf9 \cb3 \strokec9 self\cf4 \cb3 \strokec4 .model_save_path\cf5 \strokec5 )\cf4 \cb1 \strokec4 \
\cb3                 best_accuracy = val_acc\cb1 \
\
\cb3         \cf9 \cb3 \strokec9 self\cf4 \cb3 \strokec4 .model = torch.load\cf5 \strokec5 (\cf9 \cb3 \strokec9 self\cf4 \cb3 \strokec4 .model_save_path\cf5 \strokec5 )\cf4 \cb1 \strokec4 \
\cb3         \cb1 \
\cb3     \cf7 \cb3 \strokec7 def\cf4 \cb3 \strokec4  \cf8 \cb3 \strokec8 predict\cf4 \cb3 \strokec4 (\cf9 \cb3 \strokec9 self\cf4 \cb3 \strokec4 , \cf9 \cb3 \strokec9 text\cf4 \cb3 \strokec4 )\cf5 \strokec5 :\cf4 \cb1 \strokec4 \
\cb3         encoding = \cf9 \cb3 \strokec9 self\cf4 \cb3 \strokec4 .tokenizer.encode_plus\cf5 \strokec5 (\cf4 \cb1 \strokec4 \
\cb3             text\cf5 \strokec5 ,\cf4 \cb1 \strokec4 \
\cb3             add_special_tokens=\cf7 \cb3 \strokec7 True\cf5 \cb3 \strokec5 ,\cf4 \cb1 \strokec4 \
\cb3             max_length=\cf9 \cb3 \strokec9 self\cf4 \cb3 \strokec4 .max_len\cf5 \strokec5 ,\cf4 \cb1 \strokec4 \
\cb3             return_token_type_ids=\cf7 \cb3 \strokec7 False\cf5 \cb3 \strokec5 ,\cf4 \cb1 \strokec4 \
\cb3             truncation=\cf7 \cb3 \strokec7 True\cf5 \cb3 \strokec5 ,\cf4 \cb1 \strokec4 \
\cb3             padding=\cf12 \cb3 \strokec12 'max_length'\cf5 \cb3 \strokec5 ,\cf4 \cb1 \strokec4 \
\cb3             return_attention_mask=\cf7 \cb3 \strokec7 True\cf5 \cb3 \strokec5 ,\cf4 \cb1 \strokec4 \
\cb3             return_tensors=\cf12 \cb3 \strokec12 'pt'\cf5 \cb3 \strokec5 ,\cf4 \cb1 \strokec4 \
\cb3         \cf5 \strokec5 )\cf4 \cb1 \strokec4 \
\
\cb3         out = \cf5 \strokec5 \{\cf4 \cb1 \strokec4 \
\cb3               \cf12 \cb3 \strokec12 'text'\cf5 \cb3 \strokec5 :\cf4 \strokec4  text\cf5 \strokec5 ,\cf4 \cb1 \strokec4 \
\cb3               \cf12 \cb3 \strokec12 'input_ids'\cf5 \cb3 \strokec5 :\cf4 \strokec4  encoding\cf5 \strokec5 [\cf12 \cb3 \strokec12 'input_ids'\cf5 \cb3 \strokec5 ]\cf4 \strokec4 .flatten\cf5 \strokec5 (),\cf4 \cb1 \strokec4 \
\cb3               \cf12 \cb3 \strokec12 'attention_mask'\cf5 \cb3 \strokec5 :\cf4 \strokec4  encoding\cf5 \strokec5 [\cf12 \cb3 \strokec12 'attention_mask'\cf5 \cb3 \strokec5 ]\cf4 \strokec4 .flatten\cf5 \strokec5 ()\cf4 \cb1 \strokec4 \
\cb3           \cf5 \strokec5 \}\cf4 \cb1 \strokec4 \
\
\cb3         input_ids = out\cf5 \strokec5 [\cf12 \cb3 \strokec12 "input_ids"\cf5 \cb3 \strokec5 ]\cf4 \strokec4 .to\cf5 \strokec5 (\cf9 \cb3 \strokec9 self\cf4 \cb3 \strokec4 .device\cf5 \strokec5 )\cf4 \cb1 \strokec4 \
\cb3         attention_mask = out\cf5 \strokec5 [\cf12 \cb3 \strokec12 "attention_mask"\cf5 \cb3 \strokec5 ]\cf4 \strokec4 .to\cf5 \strokec5 (\cf9 \cb3 \strokec9 self\cf4 \cb3 \strokec4 .device\cf5 \strokec5 )\cf4 \cb1 \strokec4 \
\
\cb3         outputs = \cf9 \cb3 \strokec9 self\cf4 \cb3 \strokec4 .model\cf5 \strokec5 (\cf4 \cb1 \strokec4 \
\cb3             input_ids=input_ids.unsqueeze\cf5 \strokec5 (\cf10 \cb3 \strokec10 0\cf5 \cb3 \strokec5 ),\cf4 \cb1 \strokec4 \
\cb3             attention_mask=attention_mask.unsqueeze\cf5 \strokec5 (\cf10 \cb3 \strokec10 0\cf5 \cb3 \strokec5 )\cf4 \cb1 \strokec4 \
\cb3         \cf5 \strokec5 )\cf4 \cb1 \strokec4 \
\
\cb3         prediction = torch.argmax\cf5 \strokec5 (\cf4 \strokec4 outputs.logits\cf5 \strokec5 ,\cf4 \strokec4  dim=\cf10 \cb3 \strokec10 1\cf5 \cb3 \strokec5 )\cf4 \strokec4 .cpu\cf5 \strokec5 ()\cf4 \strokec4 .numpy\cf5 \strokec5 ()[\cf10 \cb3 \strokec10 0\cf5 \cb3 \strokec5 ]\cf4 \cb1 \strokec4 \
\
\cb3         \cf2 \strokec2 return\cf4 \strokec4  prediction\cb1 \
\pard\pardeftab720\partightenfactor0
\cf4 \
\
\pard\pardeftab720\partightenfactor0
\cf4 \cb3 classifier = BertClassifier\cf5 \strokec5 (\cf4 \cb1 \strokec4 \
\cb3         model_path=\cf12 \cb3 \strokec12 'cointegrated/rubert-tiny'\cf5 \cb3 \strokec5 ,\cf4 \cb1 \strokec4 \
\cb3         tokenizer_path=\cf12 \cb3 \strokec12 'cointegrated/rubert-tiny'\cf5 \cb3 \strokec5 ,\cf4 \cb1 \strokec4 \
\cb3         n_classes=\cf10 \cb3 \strokec10 2\cf5 \cb3 \strokec5 ,\cf4 \cb1 \strokec4 \
\cb3         epochs=\cf10 \cb3 \strokec10 2\cf5 \cb3 \strokec5 ,\cf4 \cb1 \strokec4 \
\cb3         model_save_path=\cf12 \cb3 \strokec12 '/content/bert.pt'\cf4 \cb1 \strokec4 \
\pard\pardeftab720\partightenfactor0
\cf5 \cb3 \strokec5 )\cf4 \cb1 \strokec4 \
\
\pard\pardeftab720\partightenfactor0
\cf4 \cb3 classifier.preparation\cf5 \strokec5 (\cf4 \cb1 \strokec4 \
\cb3         X_train=train_text\cf5 \strokec5 ,\cf4 \cb1 \strokec4 \
\cb3         y_train=train_labels\cf5 \strokec5 ,\cf4 \cb1 \strokec4 \
\cb3         X_valid=valid_text\cf5 \strokec5 ,\cf4 \cb1 \strokec4 \
\cb3         y_valid=valid_labels\cb1 \
\cb3     \cf5 \strokec5 )\cf4 \cb1 \strokec4 \
\
\cb3 classifier.train\cf5 \strokec5 ()\cf4 \cb1 \strokec4 \
}